{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: naftawayh in f:\\anaconda\\lib\\site-packages (0.4)\n",
      "Requirement already satisfied: tashaphyne in f:\\anaconda\\lib\\site-packages (from naftawayh) (0.3.4.1)\n",
      "Requirement already satisfied: pyarabic in f:\\anaconda\\lib\\site-packages (from naftawayh) (0.6.10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: qalsadi in f:\\anaconda\\lib\\site-packages (0.4.4)\n",
      "Requirement already satisfied: tashaphyne>=0.3.4.1 in f:\\anaconda\\lib\\site-packages (from qalsadi) (0.3.4.1)\n",
      "Requirement already satisfied: alyahmor>=0.1 in f:\\anaconda\\lib\\site-packages (from qalsadi) (0.1.2)\n",
      "Requirement already satisfied: naftawayh>=0.3 in f:\\anaconda\\lib\\site-packages (from qalsadi) (0.4)\n",
      "Requirement already satisfied: Arabic-Stopwords>=0.3 in f:\\anaconda\\lib\\site-packages (from qalsadi) (0.3)\n",
      "Requirement already satisfied: six>=1.10.0 in f:\\anaconda\\lib\\site-packages (from qalsadi) (1.15.0)\n",
      "Requirement already satisfied: libqutrub>=1.2.3 in f:\\anaconda\\lib\\site-packages (from qalsadi) (1.2.4.1)\n",
      "Requirement already satisfied: pyarabic>=0.6.7 in f:\\anaconda\\lib\\site-packages (from qalsadi) (0.6.10)\n",
      "Requirement already satisfied: arramooz-pysqlite>=0.3 in f:\\anaconda\\lib\\site-packages (from qalsadi) (0.3)\n",
      "Requirement already satisfied: pickledb>=0.9.2 in f:\\anaconda\\lib\\site-packages (from qalsadi) (0.9.2)\n",
      "Requirement already satisfied: future>=0.16.0 in f:\\anaconda\\lib\\site-packages (from qalsadi) (0.18.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: py-rouge in f:\\anaconda\\lib\\site-packages (1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in f:\\anaconda\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in f:\\anaconda\\lib\\site-packages (from sklearn) (0.22)\n",
      "Requirement already satisfied: joblib>=0.11 in f:\\anaconda\\lib\\site-packages (from scikit-learn->sklearn) (0.17.0)\n",
      "Requirement already satisfied: numpy>=1.11.0 in f:\\anaconda\\lib\\site-packages (from scikit-learn->sklearn) (1.19.2)\n",
      "Requirement already satisfied: scipy>=0.17.0 in f:\\anaconda\\lib\\site-packages (from scikit-learn->sklearn) (1.5.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in f:\\anaconda\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.14.5 in f:\\anaconda\\lib\\site-packages (from scipy) (1.19.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in f:\\anaconda\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in f:\\anaconda\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in f:\\anaconda\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in f:\\anaconda\\lib\\site-packages (from pandas) (1.19.2)\n",
      "Requirement already satisfied: six>=1.5 in f:\\anaconda\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in f:\\anaconda\\lib\\site-packages (3.0.5)\n",
      "Requirement already satisfied: jdcal in f:\\anaconda\\lib\\site-packages (from openpyxl) (1.4.1)\n",
      "Requirement already satisfied: et-xmlfile in f:\\anaconda\\lib\\site-packages (from openpyxl) (1.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\anaconda\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install naftawayh\n",
    "!pip install qalsadi\n",
    "!pip install future>=0.16.0\n",
    "!pip install pickledb>=0.9.2\n",
    "!pip install alyahmor>=0.1\n",
    "!pip install libqutrub>=1.2.3\n",
    "!pip install naftawayh>=0.3\n",
    "!pip install pyarabic>=0.6.7\n",
    "!pip install tashaphyne>=0.3.4.1\n",
    "!pip install arramooz-pysqlite>=0.1\n",
    "!pip install py-rouge\n",
    "!pip install sklearn\n",
    "!pip install scipy\n",
    "!pip install pandas\n",
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from matplotlib import pyplot\n",
    "from math import log2\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from scipy import stats\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score               #For goodness of fit (squared R)\n",
    "from sklearn import linear_model #For linear fitting\n",
    "from sklearn.preprocessing import PolynomialFeatures #For transforming polynomials\n",
    "\n",
    "import qalsadi.analex as qa     #For arabic preproceeing\n",
    "import naftawayh.wordtag        #For arabic preproceeing\n",
    "\n",
    "import nltk.data\n",
    "from nltk.tokenize import TreebankWordTokenizer \n",
    "\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "import itertools\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabic_preprocessing (list_of_words):\n",
    "    markers=[\".\",\",\",\"?\",\"]\",\":\",\";\",\"/\",'،',\"[\"]\n",
    "    new_list=[]\n",
    "    for i in range(len(list_of_words)):\n",
    "        flag=0\n",
    "        for j in range(len(markers)):\n",
    "            if markers[j] in list_of_words[i] and (markers[j] != list_of_words[i]):\n",
    "                new_list.append(list_of_words[i].replace(markers[j], ''))\n",
    "                flag=1\n",
    "        if flag==0 and (list_of_words[i] not in markers) and list_of_words[i]!=\"\":\n",
    "            new_list.append(list_of_words[i])\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in this part we detremine the nouns and verbs in the arabic text\n",
    "def AR_POS(Words):\n",
    "    number_of_nouns=0\n",
    "    number_of_verbs=0\n",
    "    number_of_adverbs=0\n",
    "    number_of_adjectives=0\n",
    "    tagger = naftawayh.wordtag.WordTagger()\n",
    "    debug=False;\n",
    "    limit=1000\n",
    "    nouns = []\n",
    "    verbs = []\n",
    "    adverbs = []\n",
    "    adjectives = []\n",
    "    \n",
    "    for i in range((len(Words))):\n",
    "    #       print \"--------تحليل كلمة  ------------\", word.encode('utf8');\n",
    "        analyzer = qa.Analex()\n",
    "        analyzer.set_debug(debug);\n",
    "        result = analyzer.check_text(Words[i]);\n",
    "        #print(\"res=\",result)\n",
    "        #print(repr(result[0][0]).split(\",\")[0].split(\"=\")[1])\n",
    "        #print(Words[i])\n",
    "        if (',' not in Words[i]):\n",
    "            T = repr(result[0][0]).split(\",\")[10].split(\"=\")[1]\n",
    "            #print(T)\n",
    "            #print(Words[i])\n",
    "\n",
    "            if 'noun' in T.lower():\n",
    "                #print(\"Noun\")\n",
    "                nouns.append(Words[i])\n",
    "            elif 'verb' in T.lower():\n",
    "                #print(\"Noun\")\n",
    "                verbs.append(Words[i])\n",
    "            elif 'unknown' in T.lower():\n",
    "                if tagger.is_noun(Words[i]):\n",
    "                    nouns.append(Words[i])  \n",
    "                if tagger.is_verb(Words[i]):\n",
    "                    verbs.append(Words[i])\n",
    "    \n",
    "    return nouns , verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_1 = \"PublishedCorpusFileSet/SourceTexts\"\n",
    "source_files = os.listdir(file_path_1)                                                   #source text\n",
    "\n",
    "file_path_2 = \"PublishedCorpusFileSet/PublishedEvaluation/AutomaticEvaluation\"                                  \n",
    "Automatic_evaluation_files = os.listdir(file_path_2)                                                                 \n",
    "                                                                         # Human evaluation for each candidate summary\n",
    "file_path_3 = \"PublishedCorpusFileSet/PublishedEvaluation/HumanEvaluation\"                                           \n",
    "Human_evaluation_files = os.listdir(file_path_3)                                                                     \n",
    "\n",
    "file_path_4 = \"PublishedCorpusFileSet/correctedSubmissions\"\n",
    "corrected_files = os.listdir(file_path_4)                                #candidate summary\n",
    "\n",
    "file_path_5 = \"PublishedCorpusFileSet/Arabic\"                           #Referances summary \n",
    "Arabic_files = os.listdir(file_path_5)    \n",
    "\n",
    "#print(source_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Source = []\n",
    "for file in source_files:\n",
    "    path= file_path_1 + \"/\" + file\n",
    "    with open(path,encoding='utf8') as fp:\n",
    "        source = fp.read()\n",
    "        fp.closed\n",
    "    Source.append([source])\n",
    "    \n",
    "Candidate = []\n",
    "for files in corrected_files:\n",
    "    path= file_path_4 + \"/\" + files + \"/Arabic\"\n",
    "    PATH= os.listdir(path)\n",
    "    for file in PATH:\n",
    "        Path= path + \"/\" + file\n",
    "        with open(Path,encoding='utf8') as fp:\n",
    "            candidate = fp.read()\n",
    "            fp.closed\n",
    "        Candidate.append([candidate])\n",
    "\n",
    "Reference = []\n",
    "for file in Arabic_files:\n",
    "    path= file_path_5 + \"/\" + file\n",
    "    with open(path,encoding='utf8') as fp:\n",
    "        reference = fp.read()\n",
    "        fp.closed\n",
    "    Reference.append([reference])\n",
    "       \n",
    "\n",
    "#print(Source[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_candidate=[]\n",
    "for j in range(10):\n",
    "    for i in range(j,len(Candidate),10):\n",
    "        new_candidate.append(Candidate[i])\n",
    "#print(len(new_candidate))\n",
    "#print(new_candidate[3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the human evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= file_path_3 + '/' + Human_evaluation_files[0]\n",
    "with open(path,encoding='utf8') as csvFile:\n",
    "    evaluation = csvFile.read()\n",
    "    csvFile.closed\n",
    "#print(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluation=[]\n",
    "for i in range(6,545,2):\n",
    "    Evaluation.append(int(evaluation[i]))\n",
    "#print(len(Evaluation))\n",
    "#print(Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "[3.6666666666666665, 4.0, 2.3333333333333335, 2.6666666666666665, 2.0, 3.0, 3.3333333333333335, 3.6666666666666665, 4.0, 2.6666666666666665, 3.0, 2.6666666666666665, 3.6666666666666665, 2.3333333333333335, 2.0, 3.6666666666666665, 3.3333333333333335, 3.3333333333333335, 4.666666666666667, 3.3333333333333335, 3.6666666666666665, 3.3333333333333335, 3.3333333333333335, 2.6666666666666665, 3.6666666666666665, 3.3333333333333335, 4.0, 4.333333333333333, 3.3333333333333335, 2.6666666666666665, 3.3333333333333335, 3.0, 2.3333333333333335, 2.6666666666666665, 4.666666666666667, 3.6666666666666665, 4.0, 3.6666666666666665, 3.6666666666666665, 3.6666666666666665, 3.0, 3.0, 4.0, 3.6666666666666665, 4.0, 3.6666666666666665, 3.3333333333333335, 3.0, 3.3333333333333335, 2.6666666666666665, 3.3333333333333335, 2.3333333333333335, 4.0, 4.0, 4.0, 2.6666666666666665, 3.0, 3.6666666666666665, 3.3333333333333335, 3.3333333333333335, 3.0, 3.3333333333333335, 4.0, 4.333333333333333, 2.6666666666666665, 3.3333333333333335, 4.0, 3.0, 3.3333333333333335, 3.0, 3.0, 3.3333333333333335, 3.0, 3.3333333333333335, 3.3333333333333335, 4.0, 2.6666666666666665, 3.6666666666666665, 4.0, 3.6666666666666665, 3.3333333333333335, 3.3333333333333335, 2.6666666666666665, 3.3333333333333335, 2.6666666666666665, 2.3333333333333335, 4.333333333333333, 3.3333333333333335, 4.333333333333333, 3.6666666666666665]\n"
     ]
    }
   ],
   "source": [
    "final_evaluation=[]\n",
    "\n",
    "for j in range(1,269,3):\n",
    "    final_evaluation.append((Evaluation[j-1]+Evaluation[j]+Evaluation[j+1])/3)\n",
    "print(len(final_evaluation))\n",
    "print(final_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "90\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "words_ip=[]\n",
    "num_words_ip=0\n",
    "for s in Source:\n",
    "    for sentence in s:\n",
    "        word=tokenizer.tokenize(sentence)\n",
    "        words_ip.append(word)\n",
    "print(len(words_ip))        \n",
    "for x in words_ip:\n",
    "    for y in x:\n",
    "        num_words_ip = num_words_ip +1\n",
    "\n",
    "words_cn=[]\n",
    "num_words_cn=0\n",
    "for s in new_candidate:\n",
    "    for sentence in s:\n",
    "        word=tokenizer.tokenize(sentence)\n",
    "        words_cn.append(word)\n",
    "print(len(words_cn))\n",
    "for x in words_cn:\n",
    "    for y in x:\n",
    "        num_words_cn = num_words_cn +1\n",
    "        \n",
    "words_rf=[]\n",
    "num_words_rf=0\n",
    "for s in Reference:\n",
    "    for sentence in s:\n",
    "        word=tokenizer.tokenize(sentence)\n",
    "        words_rf.append(word)\n",
    "print(len(words_rf))        \n",
    "for x in words_rf:\n",
    "    for y in x:\n",
    "        num_words_rf = num_words_rf +1\n",
    "#print(words_rf)        \n",
    "#print(num_words_ip)\n",
    "#print(num_words_cn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get number of Nouns and Verbs in the candidate summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n",
      "Fatal Error can't execute query: file: wordfrequencydictionary\n"
     ]
    }
   ],
   "source": [
    "nouns =[]\n",
    "verbs=[]\n",
    "list_of_number_of_nouns=[]\n",
    "list_of_number_of_verbs=[]\n",
    "words_cn_preprocessed=[]\n",
    "#print(words_cn[8])\n",
    "for i in words_cn:\n",
    "    words_cn_preprocessed.append(arabic_preprocessing(i))\n",
    "#print(words_cn_preprocessed[46])\n",
    "\n",
    "for i in  (words_cn_preprocessed):\n",
    "    (nouns,verbs) = AR_POS(i)\n",
    "    list_of_number_of_nouns.append(len(nouns))\n",
    "    list_of_number_of_verbs.append(len(verbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "[47, 48, 53, 37, 44, 41, 52, 38, 49, 60, 53, 61, 48, 46, 40, 59, 56, 56, 56, 56, 61, 47, 51, 48, 49, 48, 61, 75, 59, 60, 63, 56, 53, 50, 55, 64, 55, 45, 54, 35, 49, 52, 54, 53, 47, 73, 45, 47, 55, 58, 43, 69, 49, 56, 53, 43, 60, 40, 56, 44, 60, 55, 57, 50, 47, 42, 43, 48, 47, 42, 52, 62, 45, 36, 46, 41, 37, 39, 45, 40, 46, 46, 57, 54, 51, 55, 44, 51, 48, 77]\n"
     ]
    }
   ],
   "source": [
    "print(len(list_of_number_of_nouns))\n",
    "print(list_of_number_of_verbs)\n",
    "#print(words_cn_preprocessed[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the total number of words in both Reference and candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "words_ip_preprocessed=[]\n",
    "for i in words_ip:\n",
    "    words_ip_preprocessed.append(arabic_preprocessing(i))\n",
    "print(len(words_ip_preprocessed))\n",
    "\n",
    "words_rf_preprocessed=[]\n",
    "for i in words_rf:\n",
    "    words_rf_preprocessed.append(arabic_preprocessing(i))\n",
    "print(len(words_rf_preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the frequency of the words in both input and candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "final_ip_freq=[]\n",
    "final_cn_freq=[]\n",
    "\n",
    "wordlist_ip =[]\n",
    "wordlist_cn =[]\n",
    "\n",
    "j=0\n",
    "temp=[]\n",
    "for w in words_cn_preprocessed:\n",
    "    #print(\"kk=\",len(w))\n",
    "    ip_freq=[]\n",
    "    cn_freq=[]\n",
    "    #print(w)\n",
    "    #print(words_ip_preprocessed[j])\n",
    "    for i in range(len(w)):\n",
    "        if (w[i] in words_ip_preprocessed[j]) and (w[i] not in temp):\n",
    "            ip_freq.append(words_ip_preprocessed[j].count(w[i])/len(words_ip_preprocessed[j]))\n",
    "            cn_freq.append(words_cn_preprocessed[j].count(w[i])/len(words_cn_preprocessed[j]))\n",
    "            temp.append(w[i])\n",
    "            #print(w[i])\n",
    "            #print(ip_freq)\n",
    "            #print(cn_freq)\n",
    "    final_ip_freq.append(ip_freq)    \n",
    "    final_cn_freq.append(cn_freq) \n",
    "    j=j+1\n",
    "\n",
    "print(len(final_cn_freq))\n",
    "print(len(final_ip_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_number_of_words=[]\n",
    "for i in  (words_cn_preprocessed):\n",
    "    list_of_number_of_words.append(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "90\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "print(len(list_of_number_of_nouns))\n",
    "print(len(list_of_number_of_verbs))\n",
    "print(len(list_of_number_of_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the denisty of Nouns and Verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_denisty_of_nouns=[]\n",
    "final_denisty_of_verbs=[]\n",
    "for i in range(len(list_of_number_of_words)):\n",
    "    final_denisty_of_nouns.append(list_of_number_of_nouns[i]/list_of_number_of_words[i])\n",
    "    final_denisty_of_verbs.append(list_of_number_of_verbs[i]/list_of_number_of_words[i])\n",
    "#print(final_denisty_of_nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KL Divergence from input to summary and from summary to input and the smoothed version of KL: JS Divergence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p,q):\n",
    "    return sum(p[i]*log2(p[i]/q[i]) for i in range(len(p)))\n",
    "def js_divergence(p, q):\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5 * kl_divergence(p, m) + 0.5 * kl_divergence(q, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5432013208816188\n",
      "90\n",
      "0.057432460587732694\n"
     ]
    }
   ],
   "source": [
    "kl_pq=[]\n",
    "kl_qp=[]\n",
    "js_pq=[]\n",
    "\n",
    "for i in range(len(final_ip_freq)):\n",
    "    p=final_ip_freq[i]\n",
    "    q=final_cn_freq[i]\n",
    "    kl_pq.append(kl_divergence(p, q))\n",
    "    kl_qp.append(kl_divergence(q, p))\n",
    "    p = np.asarray(p)\n",
    "    q = np.asarray(q)\n",
    "    js_pq.append(js_divergence(p,q))\n",
    "print(kl_pq[0])\n",
    "print(len(kl_qp))\n",
    "print(js_pq[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2','rouge3','rouge4'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "270\n"
     ]
    }
   ],
   "source": [
    "#print(new_candidate)\n",
    "new_reference = np.reshape(Reference, (-1, 3))\n",
    "final_candidate = np.reshape(new_candidate, (-1, 9))\n",
    "\n",
    "print(len(new_reference))\n",
    "print(len(final_candidate))\n",
    "\n",
    "score1=[]\n",
    "for i in range(len(new_reference)):\n",
    "    for j in range(9):\n",
    "        for k in range(3):\n",
    "            score1.append(scorer.score( str(new_reference[i][k]) ,str(final_candidate[i][j])))\n",
    "print(len(score1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Fmeasure values in each Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmeasures_rouge1=[]\n",
    "fmeasures_rouge2=[]\n",
    "fmeasures_rouge3=[]\n",
    "fmeasures_rouge4=[]\n",
    "for i in range(270):\n",
    "    x=list(score1[i].values())\n",
    "    fmeasures_rouge1.append(x[0][2])\n",
    "    fmeasures_rouge2.append(x[1][2])\n",
    "    fmeasures_rouge3.append(x[2][2])\n",
    "    fmeasures_rouge4.append(x[3][2])\n",
    "#print(fmeasures_rouge1)\n",
    "#print(fmeasures_rouge2)\n",
    "#print(fmeasures_rouge3)\n",
    "#print(fmeasures_rouge4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "final_fmeasures_rouge1=[]\n",
    "final_fmeasures_rouge2=[]\n",
    "final_fmeasures_rouge3=[]\n",
    "final_fmeasures_rouge4=[]\n",
    "\n",
    "for j in range(1,269,3):\n",
    "    final_fmeasures_rouge1.append((fmeasures_rouge1[j-1]+fmeasures_rouge1[j]+fmeasures_rouge1[j+1])/3)\n",
    "    final_fmeasures_rouge2.append((fmeasures_rouge2[j-1]+fmeasures_rouge2[j]+fmeasures_rouge2[j+1])/3)\n",
    "    final_fmeasures_rouge3.append((fmeasures_rouge3[j-1]+fmeasures_rouge3[j]+fmeasures_rouge3[j+1])/3)\n",
    "    final_fmeasures_rouge4.append((fmeasures_rouge4[j-1]+fmeasures_rouge4[j]+fmeasures_rouge4[j+1])/3)\n",
    "    \n",
    "#print(final_fmeasures_rouge1)\n",
    "#print(final_fmeasures_rouge2)\n",
    "#print(final_fmeasures_rouge3)\n",
    "print(len(final_fmeasures_rouge4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Memo feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     0.260768\n",
      "1     0.804795\n",
      "2     0.300892\n",
      "3     0.465222\n",
      "4     0.482965\n",
      "        ...   \n",
      "85    0.331617\n",
      "86    0.420769\n",
      "87    0.327406\n",
      "88    0.368214\n",
      "89    0.229305\n",
      "Name: Memo, Length: 90, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel (r'C:\\Users\\hs\\Desktop\\MemoGrade.xlsx')\n",
    "memo=df['Memo']\n",
    "print(memo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(final_denisty_of_nouns))\n",
    "#print(len(final_denisty_of_verbs))\n",
    "\n",
    "#print(len(list_of_number_of_nouns))\n",
    "#print(len(list_of_number_of_verbs))\n",
    "\n",
    "#print(len(final_fmeasures_rouge1))\n",
    "#print(len(final_fmeasures_rouge2))\n",
    "#print(len(final_fmeasures_rouge3))\n",
    "\n",
    "#print(len(kl_pq))\n",
    "#print(len(kl_qp))\n",
    "#print(len(js_pq))\n",
    "\n",
    "#print(len(final_evaluation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 11)\n",
      "(90, 1)\n",
      "(90, 1)\n"
     ]
    }
   ],
   "source": [
    "y = np.array([final_evaluation])\n",
    "x = np.array([final_denisty_of_nouns, final_denisty_of_verbs,list_of_number_of_nouns ,list_of_number_of_verbs,final_fmeasures_rouge3, final_fmeasures_rouge4, kl_pq,kl_qp,js_pq ,final_fmeasures_rouge2 ,memo])\n",
    "\n",
    "x1=np.transpose(x)\n",
    "y1=np.transpose(y)\n",
    "\n",
    "print(x1.shape)\n",
    "print(y1.shape)\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(x1, y1)\n",
    "y_hat_linear=regr.predict(x1)\n",
    "print(y_hat_linear.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing R^2 for linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2046753312250701\n",
      "[[ 2.24266392e+01 -4.17054997e+01 -8.02878315e-02  1.84442001e-01\n",
      "   5.40527270e+00 -3.49968495e+00 -3.27992553e+01 -3.28611854e+01\n",
      "   2.86407024e+02 -7.47830878e-01 -8.38140967e-01]]\n"
     ]
    }
   ],
   "source": [
    "r2_score_1=r2_score(y1,regr.predict(x1))\n",
    "print(r2_score_1)\n",
    "print(regr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quadratic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_reg = PolynomialFeatures(degree = 2)\n",
    "X_poly = poly_reg.fit_transform(x1)\n",
    "lin_reg_2 = linear_model.LinearRegression()\n",
    "lin_reg_2.fit(X_poly, y1)\n",
    "y_hat_quad=lin_reg_2.predict(X_poly)\n",
    "#print(y_hat_quad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing R^2 for Quadratic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8747673991074338\n",
      "78\n"
     ]
    }
   ],
   "source": [
    "r2_score_1=r2_score(y1,lin_reg_2.predict(X_poly))\n",
    "print(r2_score_1)\n",
    "print(len(lin_reg_2.coef_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_1=list(itertools.chain(*y1))\n",
    "y_estimated_linear=list(itertools.chain(*y_hat_linear))\n",
    "y_estimated_quad=list(itertools.chain(*y_hat_quad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the correlation for linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kendalltau correlation: 0.330\n",
      "kendallp_value correlation: 0.000\n",
      "Pearsons correlation: 0.452\n",
      "Spearmans correlation: 0.456\n"
     ]
    }
   ],
   "source": [
    "tau, p_value = stats.kendalltau(y_1, y_estimated_linear)\n",
    "\n",
    "print ('kendalltau correlation: %.3f' % tau ) \n",
    "print ('kendallp_value correlation: %.3f' % p_value )\n",
    "\n",
    "corr, _ = pearsonr(y_1, y_estimated_linear) \n",
    "print('Pearsons correlation: %.3f' % corr)\n",
    "\n",
    "corr, _ = spearmanr(y_1, y_estimated_linear) \n",
    "print('Spearmans correlation: %.3f' % corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 11)\n",
      "(90, 1)\n"
     ]
    }
   ],
   "source": [
    "kernel = DotProduct() + WhiteKernel()\n",
    "\n",
    "print (x1.shape)\n",
    "print (y1.shape)\n",
    "\n",
    "gpr = GaussianProcessRegressor(kernel=kernel,random_state=0).fit(x1, y1)\n",
    "y_gauss=gpr.predict(x1)\n",
    "#print(y_gauss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the correlation for gaussian model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kendalltau correlation: 0.224\n",
      "kendallp_value correlation: 0.004\n",
      "Pearsons correlation: 0.295\n",
      "Spearmans correlation: 0.312\n"
     ]
    }
   ],
   "source": [
    "y_estimated_gauss=list(itertools.chain(*y_gauss))\n",
    "#Calculatin the correlation \n",
    "tau, p_value = stats.kendalltau(y_1, y_estimated_gauss)\n",
    "\n",
    "print ('kendalltau correlation: %.3f' % tau )\n",
    "print ('kendallp_value correlation: %.3f' % p_value )\n",
    "\n",
    "corr, _ = pearsonr(y_1, y_estimated_gauss)\n",
    "print('Pearsons correlation: %.3f' % corr)\n",
    "\n",
    "corr, _ = spearmanr(y_1, y_estimated_gauss)\n",
    "print('Spearmans correlation: %.3f' % corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kendalltau correlation: -0.016\n",
      "kendallp_value correlation: 0.840\n",
      "Pearsons correlation: -0.041\n",
      "Spearmans correlation: -0.027\n"
     ]
    }
   ],
   "source": [
    "#Calculatin the correlation \n",
    "tau, p_value = stats.kendalltau(y_1, kl_qp)\n",
    "\n",
    "print ('kendalltau correlation: %.3f' % tau )\n",
    "print ('kendallp_value correlation: %.3f' % p_value )\n",
    "\n",
    "corr, _ = pearsonr(y_1, kl_qp)\n",
    "print('Pearsons correlation: %.3f' % corr)\n",
    "\n",
    "corr, _ = spearmanr(y_1, kl_qp)\n",
    "print('Spearmans correlation: %.3f' % corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
